{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing some stuff, not sure what we will need yet\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "      <td>187</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>235</td>\n",
       "      <td>234</td>\n",
       "      <td>233</td>\n",
       "      <td>231</td>\n",
       "      <td>230</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>222</td>\n",
       "      <td>229</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>170</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>157</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "2      2     187     188     188     187     187     186     187     188   \n",
       "3      2     211     211     212     212     211     210     211     210   \n",
       "4     13     164     167     170     172     176     179     180     184   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "2     187  ...       202       201       200       199       198       199   \n",
       "3     210  ...       235       234       233       231       230       226   \n",
       "4     185  ...        92       105       105       108       133       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "2       198       195       194       195  \n",
       "3       225       222       229       163  \n",
       "4       157       163       164       179  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Training CSV data\n",
    "train=pd.read_csv(\"sign_mnist_train.csv\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>148</td>\n",
       "      <td>127</td>\n",
       "      <td>89</td>\n",
       "      <td>82</td>\n",
       "      <td>96</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>104</td>\n",
       "      <td>194</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>96</td>\n",
       "      <td>105</td>\n",
       "      <td>123</td>\n",
       "      <td>135</td>\n",
       "      <td>143</td>\n",
       "      <td>147</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>166</td>\n",
       "      <td>242</td>\n",
       "      <td>227</td>\n",
       "      <td>230</td>\n",
       "      <td>227</td>\n",
       "      <td>226</td>\n",
       "      <td>225</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>205</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>207</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>154</td>\n",
       "      <td>248</td>\n",
       "      <td>247</td>\n",
       "      <td>248</td>\n",
       "      <td>253</td>\n",
       "      <td>236</td>\n",
       "      <td>230</td>\n",
       "      <td>240</td>\n",
       "      <td>253</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>188</td>\n",
       "      <td>191</td>\n",
       "      <td>193</td>\n",
       "      <td>195</td>\n",
       "      <td>199</td>\n",
       "      <td>201</td>\n",
       "      <td>202</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>46</td>\n",
       "      <td>49</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      6     149     149     150     150     150     151     151     150   \n",
       "1      5     126     128     131     132     133     134     135     135   \n",
       "2     10      85      88      92      96     105     123     135     143   \n",
       "3      0     203     205     207     206     207     209     210     209   \n",
       "4      3     188     191     193     195     199     201     202     203   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     151  ...       138       148       127        89        82        96   \n",
       "1     136  ...        47       104       194       183       186       184   \n",
       "2     147  ...        68       166       242       227       230       227   \n",
       "3     210  ...       154       248       247       248       253       236   \n",
       "4     203  ...        26        40        64        48        29        46   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       106       112       120       107  \n",
       "1       184       184       182       180  \n",
       "2       226       225       224       222  \n",
       "3       230       240       253       255  \n",
       "4        49        46        46        53  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Testing CSV data\n",
    "test=pd.read_csv(\"sign_mnist_test.csv\")\n",
    "print(train.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we must seperate the labels from the data\n",
    "label_train=train[\"label\"]\n",
    "label_test=test[\"label\"]\n",
    "\n",
    "img_train=train.drop(['label'],axis=1)\n",
    "img_test=test.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing what a random image looks like in the dataset to ensure accuracy\n",
    "sampImg = random.randint(0,len(img_train.index))\n",
    "fig, ax = plt.subplots()\n",
    "i=img_train.iloc[sampImg].to_numpy()\n",
    "i=i.reshape((28,28))\n",
    "ax.imshow(i, cmap = 'gray')\n",
    "ax.set_xlabel(label_train.iloc[sampImg])\n",
    "plt.show()\n",
    "\n",
    "#A=0\n",
    "#B=1\n",
    "#C=2\n",
    "#etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the image data into a MNIST type format\n",
    "train_images = img_train.values.reshape(-1,28,28,1)\n",
    "test_images = img_test.values.reshape(-1,28,28,1)\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 and 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing label data\n",
    "labels = to_categorical(label_train, num_classes = 25)\n",
    "labels_test = to_categorical(label_test, num_classes = 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape (24709, 28, 28, 1)\n",
      "x_test shape (2746, 28, 28, 1)\n",
      "y_train shape (24709, 25)\n",
      "y_test shape (2746, 25)\n"
     ]
    }
   ],
   "source": [
    "#splitting training data to such that we can use the validation dataset for post model metrics lice confusion matrix\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_images, labels, test_size=0.10, random_state=69)\n",
    "\n",
    "print(\"x_train shape\",X_train.shape)\n",
    "print(\"x_test shape\",X_val.shape)\n",
    "print(\"y_train shape\",Y_train.shape)\n",
    "print(\"y_test shape\",Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating more data using image data generator (this will be used late, just setting it up)\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,\n",
    "    zoom_range = 0.1,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1\n",
    ")\n",
    "\n",
    "train_data = datagen.flow(X_train, Y_train, batch_size = 128)\n",
    "test_data = datagen.flow(X_val, Y_val, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 100)       2600      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 50)        45050     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 50)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 10)        4510      \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 7, 7, 10)          0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 490)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               245500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               50100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                2525      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 350285 (1.34 MB)\n",
      "Trainable params: 350285 (1.34 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture.\n",
    "model = tf.keras.models.Sequential([\n",
    "    layers.Conv2D(100, (5, 5), activation = 'relu', padding = 'same', input_shape = (28, 28, 1)),\n",
    "    layers.Conv2D(50, (3, 3), activation = 'relu', padding = 'same', input_shape = (28, 28, 1)),\n",
    "    layers.MaxPooling2D(pool_size = (2,2)),\n",
    "    layers.Conv2D(10, (3, 3), activation = 'relu', padding = 'same', input_shape = (14, 14, 1)),\n",
    "    layers.AveragePooling2D(pool_size=(2,2)),\n",
    "\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(500, activation = 'relu'),\n",
    "    layers.Dense(100, activation = 'relu'),\n",
    "    layers.Dense(25, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilinga and training the model\n",
    "model.compile(optimizer='rmsprop', # I chose this because adam would not work\n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "194/194 [==============================] - 41s 211ms/step - loss: 2.6791 - accuracy: 0.1986 - val_loss: 2.2518 - val_accuracy: 0.3077\n",
      "Epoch 2/10\n",
      "194/194 [==============================] - 41s 210ms/step - loss: 1.4994 - accuracy: 0.5158 - val_loss: 1.0393 - val_accuracy: 0.6551\n",
      "Epoch 3/10\n",
      "194/194 [==============================] - 41s 212ms/step - loss: 0.8357 - accuracy: 0.7199 - val_loss: 0.5708 - val_accuracy: 0.8084\n",
      "Epoch 4/10\n",
      "194/194 [==============================] - 42s 219ms/step - loss: 0.5126 - accuracy: 0.8271 - val_loss: 0.3514 - val_accuracy: 0.8857\n",
      "Epoch 5/10\n",
      "194/194 [==============================] - 52s 269ms/step - loss: 0.3174 - accuracy: 0.8922 - val_loss: 0.2339 - val_accuracy: 0.9235\n",
      "Epoch 6/10\n",
      "194/194 [==============================] - 46s 238ms/step - loss: 0.2303 - accuracy: 0.9236 - val_loss: 0.2308 - val_accuracy: 0.9217\n",
      "Epoch 7/10\n",
      "194/194 [==============================] - 47s 242ms/step - loss: 0.1645 - accuracy: 0.9446 - val_loss: 0.1494 - val_accuracy: 0.9501\n",
      "Epoch 8/10\n",
      "194/194 [==============================] - 46s 237ms/step - loss: 0.1237 - accuracy: 0.9587 - val_loss: 0.0989 - val_accuracy: 0.9658\n",
      "Epoch 9/10\n",
      "194/194 [==============================] - 46s 239ms/step - loss: 0.1004 - accuracy: 0.9658 - val_loss: 0.1438 - val_accuracy: 0.9527\n",
      "Epoch 10/10\n",
      "194/194 [==============================] - 46s 235ms/step - loss: 0.0812 - accuracy: 0.9722 - val_loss: 0.0661 - val_accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "trained_model = model.fit(train_data, epochs=10, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some functions to be used later\n",
    "\n",
    "#model size function\n",
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)\n",
    "\n",
    "#clustering function\n",
    "def cluster_model(model, num_clust, train_images, train_labels, epochs): \n",
    "    # ---------- Defining clustering params ----------\n",
    "    cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "\n",
    "    clustering_params = {\n",
    "    'number_of_clusters': num_clust,\n",
    "    'cluster_centroids_init': tfmot.clustering.keras.CentroidInitialization.KMEANS_PLUS_PLUS,\n",
    "    'preserve_sparsity': True\n",
    "    }\n",
    "\n",
    "    # -------  Clustering --------------\n",
    "    clustered_model = cluster_weights(model, **clustering_params)\n",
    "\n",
    "    clustered_model.compile(optimizer='rmsprop', # I chose this because adam would not work\n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "    clustered_model.fit(train_images, train_labels, epochs = epochs, validation_split =0.1)\n",
    "\n",
    "    stripped_clustered_model = tfmot.clustering.keras.strip_clustering(clustered_model)\n",
    "\n",
    "    # ------- Printing Summary ----------\n",
    "    clustered_model.summary()\n",
    "\n",
    "    return clustered_model, stripped_clustered_model\n",
    "\n",
    "def quantize_model(model, quant):\n",
    "\n",
    "    #-------------Quantizing------------------\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    if quant == 16: #using float 16 if requested\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "        quantized_tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "    else: #using 8bit\n",
    "        quantized_tflite_model = converter.convert()\n",
    "        \n",
    "    # with open(f'quantized_model.tflite', 'wb') as f:\n",
    "    #     f.write(quantized_tflite_model)\n",
    "    # interpreter_quant = tf.lite.Interpreter(model_path=str(f'quantized_model.tflite'))\n",
    "    # interpreter_quant.allocate_tensors()     \n",
    "\n",
    "\n",
    "\n",
    "    return quantized_tflite_model\n",
    "\n",
    "def iterative_prune_model(model, initial_sparsity, final_sparsity, begin_step, end_step, train_images, train_labels, epochs):\n",
    "  # --------- Pruning Variable Setup ---------\n",
    "  prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "  # Define model for pruning.\n",
    "  pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=initial_sparsity,\n",
    "        final_sparsity=final_sparsity, begin_step=begin_step, end_step=end_step, frequency=100)\n",
    "  }\n",
    "\n",
    "  # ------------- Pruning Model -------------\n",
    "  pruned_model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "  # `prune_low_magnitude` requires a recompile.\n",
    "  pruned_model.compile(optimizer='rmsprop', # I chose this because adam would not work\n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "  callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  ]\n",
    "\n",
    "  pruned_model.fit(train_images, train_labels, epochs=epochs, validation_split=0.1,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  # -------- Strip Pruning -------------\n",
    "  stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "  # ------- Printing Summaries ----------\n",
    "  pruned_model.summary()\n",
    "  stripped_pruned_model.summary()\n",
    "\n",
    "  return pruned_model, stripped_pruned_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "model.save('untrained_signLang_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 4s - loss: 0.0496 - accuracy: 0.9856 - 4s/epoch - 20ms/step\n",
      "225/225 [==============================] - 5s 20ms/step\n",
      "Model Size: 2358050.00 bytes\n",
      "Model Accuracy: 98.56386184692383 %\n",
      "Inference Time is 0.0006865563169246403 s\n"
     ]
    }
   ],
   "source": [
    "#getting baseline numbers to understand model.\n",
    "model = tf.keras.models.load_model('trained_signLang_model.h5')\n",
    "\n",
    "# Evaluate prediction accuracy\n",
    "test_loss, test_acc = model.evaluate(test_images, labels_test, verbose=2)\n",
    "\n",
    "# Evaluate Inference Time\n",
    "startTime = time.time()\n",
    "prediction = model.predict(test_images)\n",
    "executionTime = (time.time() - startTime)/len(test_images)\n",
    "\n",
    "#Printing the model size and inference time and accuracy\n",
    "print(\"Model Size: %.2f bytes\" % (get_gzipped_model_size('untrained_signLang_model.h5')))\n",
    "print('Model Accuracy:', test_acc*100, '%')\n",
    "print(\"Inference Time is\", executionTime, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "773/773 [==============================] - 61s 74ms/step - loss: 0.8315 - accuracy: 0.7419 - val_loss: 0.3689 - val_accuracy: 0.9472\n",
      "Epoch 2/2\n",
      "773/773 [==============================] - 60s 78ms/step - loss: 0.1632 - accuracy: 0.9957 - val_loss: 0.0761 - val_accuracy: 1.0000\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d  (None, 28, 28, 100)       5102      \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 28, 28, 50)        90052     \n",
      " _1 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_max_po  (None, 14, 14, 50)        1         \n",
      " oling2d (PruneLowMagnitude                                      \n",
      " )                                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d  (None, 14, 14, 10)        9012      \n",
      " _2 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_averag  (None, 7, 7, 10)          1         \n",
      " e_pooling2d (PruneLowMagni                                      \n",
      " tude)                                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_flatte  (None, 490)               1         \n",
      " n (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense   (None, 500)               490502    \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 100)               100102    \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_  (None, 25)                5027      \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 699800 (2.67 MB)\n",
      "Trainable params: 350285 (1.34 MB)\n",
      "Non-trainable params: 349515 (1.33 MB)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 100)       2600      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 50)        45050     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 14, 14, 50)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 14, 14, 10)        4510      \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 7, 7, 10)          0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 490)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               245500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               50100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25)                2525      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 350285 (1.34 MB)\n",
      "Trainable params: 350285 (1.34 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# first thing we will do to reduce model size is prune it\n",
    "\n",
    "# load model\n",
    "loaded_model = tf.keras.models.load_model('trained_signLang_model.h5')\n",
    "\n",
    "#variables that are to be manipulated\n",
    "initial_sparsity = 0.1\n",
    "final_sparsity = 0.9\n",
    "begin_step = 0\n",
    "end_step = 100\n",
    "epochs = 2\n",
    "\n",
    "#function call\n",
    "pruned_model, stripped_pruned_model = iterative_prune_model(loaded_model, initial_sparsity, final_sparsity, \n",
    "                                                            begin_step, end_step, train_images, \n",
    "                                                            labels, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mnorr\\Anaconda3\\envs\\ensf-ml\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "#saving model\n",
    "stripped_pruned_model.save('stripPruned_signLang_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d/kernel:0: 90.00% sparsity  (2250/2500)\n",
      "conv2d_1/kernel:0: 90.00% sparsity  (40500/45000)\n",
      "conv2d_2/kernel:0: 90.00% sparsity  (4050/4500)\n",
      "dense/kernel:0: 90.00% sparsity  (220500/245000)\n",
      "dense_1/kernel:0: 90.00% sparsity  (45000/50000)\n",
      "dense_2/kernel:0: 90.00% sparsity  (2250/2500)\n"
     ]
    }
   ],
   "source": [
    "def print_model_weights_sparsity(model):\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            if \"kernel\" not in weight.name or \"centroid\" in weight.name:\n",
    "                continue\n",
    "            weight_size = weight.numpy().size\n",
    "            zero_num = np.count_nonzero(weight == 0)\n",
    "            print(\n",
    "                f\"{weight.name}: {zero_num/weight_size:.2%} sparsity \",\n",
    "                f\"({zero_num}/{weight_size})\",\n",
    "            )\n",
    "print_model_weights_sparsity(stripped_pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 4s - loss: 0.4194 - accuracy: 0.8854 - 4s/epoch - 19ms/step\n",
      "225/225 [==============================] - 5s 20ms/step\n",
      "Model Size: 271914.00 bytes\n",
      "Model Accuracy: 88.53875994682312 %\n",
      "Inference Time is 0.0006623558006137693 s\n"
     ]
    }
   ],
   "source": [
    "#getting baseline numbers to understand model.\n",
    "s_model = pruned_model\n",
    "# Evaluate prediction accuracy\n",
    "test_loss, test_acc = s_model.evaluate(test_images, labels_test, verbose=2)\n",
    "\n",
    "# Evaluate Inference Time\n",
    "startTime = time.time()\n",
    "prediction = s_model.predict(test_images)\n",
    "executionTime = (time.time() - startTime)/len(test_images)\n",
    "\n",
    "#Printing the model size and inference time and accuracy\n",
    "print(\"Model Size: %.2f bytes\" % (get_gzipped_model_size('stripPruned_signLang_model.h5')))\n",
    "print('Model Accuracy:', test_acc*100, '%')\n",
    "print(\"Inference Time is\", executionTime, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "773/773 [==============================] - 80s 99ms/step - loss: 0.0972 - accuracy: 0.9953 - val_loss: 0.0569 - val_accuracy: 0.9996\n",
      "Epoch 2/2\n",
      "773/773 [==============================] - 74s 95ms/step - loss: 0.0320 - accuracy: 0.9996 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cluster_conv2d (ClusterWei  (None, 28, 28, 100)       5116      \n",
      " ghts)                                                           \n",
      "                                                                 \n",
      " cluster_conv2d_1 (ClusterW  (None, 28, 28, 50)        90066     \n",
      " eights)                                                         \n",
      "                                                                 \n",
      " cluster_max_pooling2d (Clu  (None, 14, 14, 50)        0         \n",
      " sterWeights)                                                    \n",
      "                                                                 \n",
      " cluster_conv2d_2 (ClusterW  (None, 14, 14, 10)        9026      \n",
      " eights)                                                         \n",
      "                                                                 \n",
      " cluster_average_pooling2d   (None, 7, 7, 10)          0         \n",
      " (ClusterWeights)                                                \n",
      "                                                                 \n",
      " cluster_flatten (ClusterWe  (None, 490)               0         \n",
      " ights)                                                          \n",
      "                                                                 \n",
      " cluster_dense (ClusterWeig  (None, 500)               490516    \n",
      " hts)                                                            \n",
      "                                                                 \n",
      " cluster_dense_1 (ClusterWe  (None, 100)               100116    \n",
      " ights)                                                          \n",
      "                                                                 \n",
      " cluster_dense_2 (ClusterWe  (None, 25)                5041      \n",
      " ights)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 699881 (4.00 MB)\n",
      "Trainable params: 350381 (1.34 MB)\n",
      "Non-trainable params: 349500 (2.67 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable Sequential object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m num_clust \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#Function call\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m clustered_model, strip_clust_model \u001b[38;5;241m=\u001b[39m cluster_model(model, num_clust, train_images, labels, epochs)\n\u001b[0;32m     11\u001b[0m strip_clust_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstripClust_signLang_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable Sequential object"
     ]
    }
   ],
   "source": [
    "#clustering the pruned model and perserving its sparcity\n",
    "\n",
    "model = tf.keras.models.load_model('stripPruned_signLang_model.h5')\n",
    "\n",
    "#defining some variables\n",
    "num_clust = 16\n",
    "\n",
    "#Function call\n",
    "clustered_model, strip_clust_model = cluster_model(model, num_clust, train_images, labels, epochs)\n",
    "\n",
    "strip_clust_model.save('stripClust_signLang_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting baseline numbers to understand model.\n",
    "c_model = cluster_model\n",
    "# Evaluate prediction accuracy\n",
    "test_loss, test_acc = c_model.evaluate(test_images, labels_test, verbose=2)\n",
    "\n",
    "# Evaluate Inference Time\n",
    "startTime = time.time()\n",
    "prediction = c_model.predict(test_images)\n",
    "executionTime = (time.time() - startTime)/len(test_images)\n",
    "\n",
    "#Printing the model size and inference time and accuracy\n",
    "print(\"Model Size: %.2f bytes\" % (get_gzipped_model_size('stripClust_signLang_model.h5')))\n",
    "print('Model Accuracy:', test_acc*100, '%')\n",
    "print(\"Inference Time is\", executionTime, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\mnorr\\AppData\\Local\\Temp\\tmpkktnwbz7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\mnorr\\AppData\\Local\\Temp\\tmpkktnwbz7\\assets\n"
     ]
    }
   ],
   "source": [
    "# Next we will quantize the pruned model\n",
    "\n",
    "#loading model\n",
    "model = tf.keras.models.load_model('stripPruned_signLang_model.h5')\n",
    "\n",
    "#Defining some new variables\n",
    "quant = 8\n",
    "\n",
    "#function call\n",
    "quantized_tflite_model = quantize_model(model, quant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensf-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
